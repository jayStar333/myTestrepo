```python
def define_table_directly(args: argparse.Namespace):
    """
    Define a table directly with primary key information using Oracle metadata.
    
    :param args: Command line arguments containing schema and table information
    :return: None
    """
    source_identifier = ".".join([args.schema, args.table])
    query = f"SELECT * from {source_identifier}"
    print(query)
    
    authenticator = OracleAuthentication(**asdict(GL_SOURCES[args.source_of_record]))
    
    try:
        source_table = OracleTable(
            query=query,
            schema=args.schema,
            table_name=args.table,
            authenticator=authenticator
        )
        
        # Query to get primary key columns for this table
        pk_query = f"""
            SELECT column_name
            FROM all_cons_columns
            WHERE constraint_name = (
                SELECT constraint_name
                FROM all_constraints
                WHERE table_name = '{args.table}'
                AND owner = '{args.schema}'
                AND constraint_type = 'P'
            )
        """
        # Execute PK query using the same connection
        pk_columns = set(row[0] for row in source_table.connection.execute(pk_query))
        
    except AttributeError as error:
        print(f"{error=}")
        return

    print("before getting view")
    try:
        source_table.get_view_definition_table(schema=source_identifier)
    except Exception as error:
        print(f"{error=}")
        return

    # Include PK information in the output
    columns_info = []
    for column in source_table.all_columns:
        is_pk = "(PK)" if column.name in pk_columns else ""
        columns_info.append(f"{column.name}{is_pk}")

    print(f"table={source_identifier}|columns={len(source_table.all_columns)}|details={', '.join(columns_info)}")
    print("after getting view")
```
###################################
import os
import csv
import pandas as pd
from datetime import datetime

def process_csv_file(
    input_file_path, 
    output_directory, 
    selected_fields, 
    field_rename_mapping, 
    additional_field_generators
):
    """
    Process a CSV file by selecting specific fields, renaming them, 
    and adding derived fields.
    
    Parameters:
    - input_file_path (str): Full path to the input CSV file
    - output_directory (str): Directory where the output file will be saved
    - selected_fields (list): List of fields to extract from the original CSV
    - field_rename_mapping (dict): Mapping of old field names to new field names
    - additional_field_generators (list): List of functions to generate new fields
    
    Returns:
    - str: Path to the created metadata CSV file
    """
    # Ensure the output directory exists
    os.makedirs(output_directory, exist_ok=True)
    
    # Read the input CSV file
    try:
        df = pd.read_csv(input_file_path)
    except Exception as e:
        raise ValueError(f"Error reading input CSV file: {e}")
    
    # Validate and select specified fields
    selected_df = df[selected_fields].copy()
    
    # Rename fields
    selected_df.rename(columns=field_rename_mapping, inplace=True)
    
    # Generate additional fields
    for field_generator in additional_field_generators:
        field_name, generator_func = field_generator
        selected_df[field_name] = selected_df.apply(generator_func, axis=1)
    
    # Create metadata filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    metadata_filename = f"metadata_{timestamp}.csv"
    metadata_filepath = os.path.join(output_directory, metadata_filename)
    
    # Write the processed data to CSV
    selected_df.to_csv(metadata_filepath, index=False)
    
    return metadata_filepath

# Example usage and demonstration
def example_usage():
    # Example input file path
    input_file = 'sample_data.csv'
    
    # Output directory
    output_dir = './processed_data'
    
    # Selected fields to extract
    selected_fields = ['name', 'age', 'salary', 'department']
    
    # Field renaming mapping
    field_rename_map = {
        'name': 'full_name', 
        'age': 'person_age', 
        'salary': 'annual_income',
        'department': 'work_unit'
    }
    
    # Additional field generators
    def calculate_tax_bracket(row):
        """Calculate tax bracket based on income"""
        salary = row['annual_income']
        if salary < 30000:
            return 'Low'
        elif 30000 <= salary < 80000:
            return 'Medium'
        else:
            return 'High'
    
    def generate_age_category(row):
        """Categorize age"""
        age = row['person_age']
        if age < 25:
            return 'Young'
        elif 25 <= age < 45:
            return 'Mid-Career'
        else:
            return 'Senior'
    
    def calculate_potential_bonus(row):
        """Calculate potential bonus based on age and income"""
        age_category = generate_age_category(row)
        tax_bracket = calculate_tax_bracket(row)
        
        if age_category == 'Senior' and tax_bracket == 'High':
            return row['annual_income'] * 0.15
        elif age_category == 'Mid-Career' and tax_bracket == 'Medium':
            return row['annual_income'] * 0.10
        else:
            return row['annual_income'] * 0.05
    
    def generate_employee_code(row):
        """Generate a unique employee code"""
        return f"{row['work_unit'][:3].upper()}-{row['person_age']}"
    
    # Additional field generator specification
    additional_fields = [
        ('tax_bracket', calculate_tax_bracket),
        ('age_category', generate_age_category),
        ('potential_bonus', calculate_potential_bonus),
        ('employee_code', generate_employee_code)
    ]
    
    # Process the CSV
    output_file = process_csv_file(
        input_file, 
        output_dir, 
        selected_fields, 
        field_rename_map, 
        additional_fields
    )
    
    print(f"Processed CSV saved to: {output_file}")

# Uncomment the line below to run the example
# example_usage()
###################################
import os
import csv
import pandas as pd
from datetime import datetime

def get_hive_datatype(oracle_type):
    """
    Convert Oracle datatype to Hive equivalent
    """
    oracle_type = str(oracle_type).upper().strip()
    
    # Mapping of Oracle to Hive datatypes
    datatype_mapping = {
        'VARCHAR2': 'STRING',
        'VARCHAR': 'STRING',
        'CHAR': 'STRING',
        'NCHAR': 'STRING',
        'NVARCHAR2': 'STRING',
        'CLOB': 'STRING',
        'NCLOB': 'STRING',
        'NUMBER': 'DECIMAL',
        'NUMERIC': 'DECIMAL',
        'DECIMAL': 'DECIMAL',
        'INTEGER': 'INT',
        'INT': 'INT',
        'SMALLINT': 'SMALLINT',
        'FLOAT': 'DOUBLE',
        'REAL': 'DOUBLE',
        'DOUBLE PRECISION': 'DOUBLE',
        'DATE': 'TIMESTAMP',
        'TIMESTAMP': 'TIMESTAMP',
        'BLOB': 'BINARY',
        'LONG': 'STRING',
        'LONG RAW': 'BINARY',
        'RAW': 'BINARY',
        'BOOLEAN': 'BOOLEAN'
    }
    
    # Handle NUMBER with precision and scale
    if 'NUMBER' in oracle_type:
        if '(' in oracle_type:
            precision_scale = oracle_type.split('(')[1].rstrip(')').split(',')
            if len(precision_scale) == 2:
                precision, scale = map(int, precision_scale)
                if scale == 0:
                    if precision < 5:
                        return 'SMALLINT'
                    elif precision < 10:
                        return 'INT'
                    elif precision < 19:
                        return 'BIGINT'
                    else:
                        return 'DECIMAL({},{})'.format(precision, scale)
                else:
                    return 'DECIMAL({},{})'.format(precision, scale)
    
    # Return mapped type or STRING as default
    return datatype_mapping.get(oracle_type.split('(')[0], 'STRING')

def process_csv_file(
    input_file_path, 
    output_directory, 
    selected_fields,
    field_rename_mapping,
    dev_prefix_field,      # Field to add dev_ prefix
    conf_prefix_field,     # Field to add conf_ prefix
    pass_through_field,    # Field to pass through as-is
    datatype_field        # Field containing Oracle datatype
):
    """
    Process a CSV file by selecting specific fields, renaming them, 
    and adding derived fields with specific prefixes and transformations.
    
    Parameters:
    - input_file_path (str): Full path to the input CSV file
    - output_directory (str): Directory where the output file will be saved
    - selected_fields (list): List of fields to extract from the original CSV
    - field_rename_mapping (dict): Mapping of old field names to new field names
    - dev_prefix_field (str): Field to be prefixed with 'dev_'
    - conf_prefix_field (str): Field to be prefixed with 'conf_'
    - pass_through_field (str): Field to be passed through unchanged
    - datatype_field (str): Field containing Oracle datatype to be converted to Hive
    
    Returns:
    - str: Path to the created metadata CSV file
    """
    # Ensure the output directory exists
    os.makedirs(output_directory, exist_ok=True)
    
    try:
        # Read the input CSV file
        df = pd.read_csv(input_file_path)
        
        # Validate and select specified fields
        selected_df = df[selected_fields].copy()
        
        # Rename fields
        selected_df.rename(columns=field_rename_mapping, inplace=True)
        
        # Add dev_ prefix column
        if dev_prefix_field in selected_df.columns:
            new_dev_field = f'dev_{dev_prefix_field}'
            selected_df[new_dev_field] = 'dev_' + selected_df[dev_prefix_field].astype(str)
        
        # Add conf_ prefix column
        if conf_prefix_field in selected_df.columns:
            new_conf_field = f'conf_{conf_prefix_field}'
            selected_df[new_conf_field] = 'conf_' + selected_df[conf_prefix_field].astype(str)
        
        # Add pass-through column
        if pass_through_field in selected_df.columns:
            new_pass_field = f'new_{pass_through_field}'
            selected_df[new_pass_field] = selected_df[pass_through_field]
        
        # Add Hive datatype column
        if datatype_field in selected_df.columns:
            new_type_field = f'hive_{datatype_field}'
            selected_df[new_type_field] = selected_df[datatype_field].apply(get_hive_datatype)
        
        # Create metadata filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        metadata_filename = f"metadata_{timestamp}.csv"
        metadata_filepath = os.path.join(output_directory, metadata_filename)
        
        # Write the processed data to CSV
        selected_df.to_csv(metadata_filepath, index=False)
        
        return metadata_filepath
        
    except Exception as e:
        raise ValueError(f"Error processing CSV file: {e}")

# Example usage and demonstration
def example_usage():
    # Example input file path
    input_file = 'sample_data.csv'
    
    # Output directory
    output_dir = './processed_data'
    
    # Selected fields to extract
    selected_fields = ['column_name', 'data_type', 'table_name', 'schema_name', 'column_id']
    
    # Field renaming mapping
    field_rename_map = {
        'column_name': 'col_name',
        'data_type': 'oracle_type',
        'table_name': 'tbl_name',
        'schema_name': 'schema',
        'column_id': 'col_id'
    }
    
    # Process the CSV
    output_file = process_csv_file(
        input_file_path=input_file,
        output_directory=output_dir,
        selected_fields=selected_fields,
        field_rename_mapping=field_rename_map,
        dev_prefix_field='tbl_name',    # Will create 'dev_tbl_name'
        conf_prefix_field='schema',      # Will create 'conf_schema'
        pass_through_field='col_name',   # Will create 'new_col_name'
        datatype_field='oracle_type'     # Will create 'hive_oracle_type'
    )
    
    print(f"Processed CSV saved to: {output_file}")

# Example of input CSV structure:
"""
column_name,data_type,table_name,schema_name,column_id
id,NUMBER(10),customers,sales,1
name,VARCHAR2(100),customers,sales,2
email,VARCHAR2(255),customers,sales,3
created_date,TIMESTAMP,customers,sales,4
balance,NUMBER(10,2),customers,sales,5
"""

# Example of output CSV structure:
"""
col_name,oracle_type,tbl_name,schema,col_id,dev_tbl_name,conf_schema,new_col_name,hive_oracle_type
id,NUMBER(10),customers,sales,1,dev_customers,conf_sales,id,INT
name,VARCHAR2(100),customers,sales,2,dev_customers,conf_sales,name,STRING
email,VARCHAR2(255),customers,sales,3,dev_customers,conf_sales,email,STRING
created_date,TIMESTAMP,customers,sales,4,dev_customers,conf_sales,created_date,TIMESTAMP
balance,NUMBER(10,2),customers,sales,5,dev_customers,conf_sales,balance,DECIMAL(10,2)
"""

# Uncomment the line below to run the example
# example_usage()
###################################
import os
import csv
import pandas as pd
from datetime import datetime

def get_hive_datatype(oracle_type):
    """
    Convert Oracle datatype to Hive equivalent
    """
    oracle_type = str(oracle_type).upper().strip()
    
    # Mapping of Oracle to Hive datatypes
    datatype_mapping = {
        'VARCHAR2': 'STRING',
        'VARCHAR': 'STRING',
        'CHAR': 'STRING',
        'NCHAR': 'STRING',
        'NVARCHAR2': 'STRING',
        'CLOB': 'STRING',
        'NCLOB': 'STRING',
        'NUMBER': 'DECIMAL',
        'NUMERIC': 'DECIMAL',
        'DECIMAL': 'DECIMAL',
        'INTEGER': 'INT',
        'INT': 'INT',
        'SMALLINT': 'SMALLINT',
        'FLOAT': 'DOUBLE',
        'REAL': 'DOUBLE',
        'DOUBLE PRECISION': 'DOUBLE',
        'DATE': 'TIMESTAMP',
        'TIMESTAMP': 'TIMESTAMP',
        'BLOB': 'BINARY',
        'LONG': 'STRING',
        'LONG RAW': 'BINARY',
        'RAW': 'BINARY',
        'BOOLEAN': 'BOOLEAN'
    }
    
    # Handle NUMBER with precision and scale
    if 'NUMBER' in oracle_type:
        if '(' in oracle_type:
            precision_scale = oracle_type.split('(')[1].rstrip(')').split(',')
            if len(precision_scale) == 2:
                precision, scale = map(int, precision_scale)
                if scale == 0:
                    if precision < 5:
                        return 'SMALLINT'
                    elif precision < 10:
                        return 'INT'
                    elif precision < 19:
                        return 'BIGINT'
                    else:
                        return 'DECIMAL({},{})'.format(precision, scale)
                else:
                    return 'DECIMAL({},{})'.format(precision, scale)
    
    # Return mapped type or STRING as default
    return datatype_mapping.get(oracle_type.split('(')[0], 'STRING')

def modify_with_dev_conf(value):
    """
    Modify string by:
    1. Adding 'dev_' prefix
    2. Replacing text after last underscore with 'conf'
    
    Example:
    'table_name_prod' -> 'dev_table_name_conf'
    'simple_table' -> 'dev_simple_conf'
    'table' -> 'dev_table_conf'
    """
    # Add dev_ prefix
    modified = f"dev_{value}"
    
    # Split the string by underscores
    parts = modified.split('_')
    
    # If there's only one part after adding dev_, add _conf
    if len(parts) == 2:
        return f"{parts[0]}_{parts[1]}_conf"
    
    # Replace the last part with 'conf'
    parts[-1] = 'conf'
    
    # Join back together
    return '_'.join(parts)

def process_csv_file(
    input_file_path, 
    output_directory, 
    selected_fields,
    field_rename_mapping,
    dev_prefix_field,      # Field to add dev_ prefix and replace last part with conf
    conf_prefix_field,     # Field to add conf_ prefix
    pass_through_field,    # Field to pass through as-is
    datatype_field        # Field containing Oracle datatype
):
    """
    Process a CSV file by selecting specific fields, renaming them, 
    and adding derived fields with specific prefixes and transformations.
    
    Parameters:
    - input_file_path (str): Full path to the input CSV file
    - output_directory (str): Directory where the output file will be saved
    - selected_fields (list): List of fields to extract from the original CSV
    - field_rename_mapping (dict): Mapping of old field names to new field names
    - dev_prefix_field (str): Field to be modified with dev_ prefix and _conf suffix
    - conf_prefix_field (str): Field to be prefixed with 'conf_'
    - pass_through_field (str): Field to be passed through unchanged
    - datatype_field (str): Field containing Oracle datatype to be converted to Hive
    
    Returns:
    - str: Path to the created metadata CSV file
    """
    # Ensure the output directory exists
    os.makedirs(output_directory, exist_ok=True)
    
    try:
        # Read the input CSV file
        df = pd.read_csv(input_file_path)
        
        # Validate and select specified fields
        selected_df = df[selected_fields].copy()
        
        # Rename fields
        selected_df.rename(columns=field_rename_mapping, inplace=True)
        
        # Add modified dev_ field with _conf suffix
        if dev_prefix_field in selected_df.columns:
            new_dev_field = f'dev_conf_{dev_prefix_field}'
            selected_df[new_dev_field] = selected_df[dev_prefix_field].apply(modify_with_dev_conf)
        
        # Add conf_ prefix column
        if conf_prefix_field in selected_df.columns:
            new_conf_field = f'conf_{conf_prefix_field}'
            selected_df[new_conf_field] = 'conf_' + selected_df[conf_prefix_field].astype(str)
        
        # Add pass-through column
        if pass_through_field in selected_df.columns:
            new_pass_field = f'new_{pass_through_field}'
            selected_df[new_pass_field] = selected_df[pass_through_field]
        
        # Add Hive datatype column
        if datatype_field in selected_df.columns:
            new_type_field = f'hive_{datatype_field}'
            selected_df[new_type_field] = selected_df[datatype_field].apply(get_hive_datatype)
        
        # Create metadata filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        metadata_filename = f"metadata_{timestamp}.csv"
        metadata_filepath = os.path.join(output_directory, metadata_filename)
        
        # Write the processed data to CSV
        selected_df.to_csv(metadata_filepath, index=False)
        
        return metadata_filepath
        
    except Exception as e:
        raise ValueError(f"Error processing CSV file: {e}")

# Example usage and demonstration
def example_usage():
    # Example input file path
    input_file = 'sample_data.csv'
    
    # Output directory
    output_dir = './processed_data'
    
    # Selected fields to extract
    selected_fields = ['column_name', 'data_type', 'table_name', 'schema_name', 'column_id']
    
    # Field renaming mapping
    field_rename_map = {
        'column_name': 'col_name',
        'data_type': 'oracle_type',
        'table_name': 'tbl_name',
        'schema_name': 'schema',
        'column_id': 'col_id'
    }
    
    # Process the CSV
    output_file = process_csv_file(
        input_file_path=input_file,
        output_directory=output_dir,
        selected_fields=selected_fields,
        field_rename_mapping=field_rename_map,
        dev_prefix_field='tbl_name',    # Will create 'dev_conf_tbl_name'
        conf_prefix_field='schema',      # Will create 'conf_schema'
        pass_through_field='col_name',   # Will create 'new_col_name'
        datatype_field='oracle_type'     # Will create 'hive_oracle_type'
    )
    
    print(f"Processed CSV saved to: {output_file}")

# Example of input CSV structure:
"""
column_name,data_type,table_name,schema_name,column_id
id,NUMBER(10),customers_prod,sales,1
name,VARCHAR2(100),customers_prod,sales,2
email,VARCHAR2(255),customers_test,sales,3
created_date,TIMESTAMP,customers_dev,sales,4
balance,NUMBER(10,2),customers_prod,sales,5
"""

# Example of output CSV structure:
"""
col_name,oracle_type,tbl_name,schema,col_id,dev_conf_tbl_name,conf_schema,new_col_name,hive_oracle_type
id,NUMBER(10),customers_prod,sales,1,dev_customers_conf,conf_sales,id,INT
name,VARCHAR2(100),customers_prod,sales,2,dev_customers_conf,conf_sales,name,STRING
email,VARCHAR2(255),customers_test,sales,3,dev_customers_conf,conf_sales,email,STRING
created_date,TIMESTAMP,customers_dev,sales,4,dev_customers_conf,conf_sales,created_date,TIMESTAMP
balance,NUMBER(10,2),customers_prod,sales,5,dev_customers_conf,conf_sales,balance,DECIMAL(10,2)
"""

# Uncomment the line below to run the example
# example_usage()

#############################################################
from dataclasses import dataclass, field
from datetime import datetime
from typing import List, Optional

@dataclass
class OracleColumn(RelationalDBSourceColumn):
    field_format: str = ""
    field_length: str = ""
    decimal_length: str = ""
    field_precision: str = ""
    column_id: int = 0
    column_position: int = 0
    column_name: str = ""
    column_key: str = ""
    schema_name: str = ""
    table_name: str = ""
    
    # VARCHAR specific metadata
    char_length: int = 0
    char_used: str = "BYTE"  # BYTE or CHAR
    byte_length: int = 0
    max_length: int = 4000  # Oracle VARCHAR2 default max length
    char_semantics: bool = False  # False for BYTE, True for CHAR semantics
    
    # Additional metadata
    created_date: datetime = None
    modified_date: datetime = None
    created_by: str = ""
    modified_by: str = ""
    comments: str = ""
    
    def __post_init__(self):
        self.field_name = self.field_name.upper()
        self.column_name = self.column_name.upper() if self.column_name else self.field_name
        self.schema_name = self.schema_name.upper() if self.schema_name else ""
        
        if not self.column_key:
            self._generate_column_key()
            
        if self.field_type:
            try:
                self.field = oracle_field_factory.get_field()
                if self._is_varchar_type():
                    self._setup_varchar_metadata()
            except TypeError as error:
                raise TypeError(f"Field {self.field_type} not found in field type definitions.")
    
    def _is_varchar_type(self) -> bool:
        """Check if the field type is VARCHAR or VARCHAR2"""
        return self.field_type.upper() in ["VARCHAR", "VARCHAR2"]
    
    def _setup_varchar_metadata(self):
        """Setup VARCHAR specific metadata"""
        if self._is_varchar_type():
            # Convert field_length to integer if it exists
            if self.field_length:
                length = int(self.field_length)
                if self.char_semantics:
                    self.char_length = length
                    # Estimate byte length (assuming UTF-8 encoding)
                    self.byte_length = length * 4
                else:
                    self.byte_length = length
                    # Estimate char length (assuming UTF-8 encoding)
                    self.char_length = length // 4
                    
                # Validate against maximum length
                if self.byte_length > self.max_length:
                    raise ValueError(f"VARCHAR length exceeds maximum allowed ({self.max_length} bytes)")
    
    def validate_varchar_constraints(self):
        """Validate VARCHAR specific constraints"""
        if self._is_varchar_type():
            if not self.field_length:
                raise ValueError("VARCHAR columns must specify a length")
            
            try:
                length = int(self.field_length)
                if length <= 0:
                    raise ValueError("VARCHAR length must be positive")
                if length > self.max_length:
                    raise ValueError(f"VARCHAR length exceeds maximum allowed ({self.max_length})")
            except ValueError as e:
                raise ValueError(f"Invalid VARCHAR length: {self.field_length}")
            
            if self.char_used not in ["BYTE", "CHAR"]:
                raise ValueError("char_used must be either 'BYTE' or 'CHAR'")
    
    def get_varchar_metadata(self) -> dict:
        """Get VARCHAR specific metadata"""
        if not self._is_varchar_type():
            return {}
            
        return {
            'char_length': self.char_length,
            'byte_length': self.byte_length,
            'char_used': self.char_used,
            'char_semantics': self.char_semantics,
            'max_length': self.max_length
        }
    
    def get_ddl_definition(self) -> str:
        """Generate DDL for the column"""
        ddl_parts = [
            self.field_name,
            self.field_type
        ]
        
        if self._is_varchar_type():
            semantics = "CHAR" if self.char_semantics else "BYTE"
            ddl_parts[1] += f"({self.field_length} {semantics})"
        
        return " ".join(ddl_parts)
    
    def estimate_storage_bytes(self) -> int:
        """Estimate storage bytes required for VARCHAR column"""
        if not self._is_varchar_type():
            return 0
            
        # Basic estimation - actual storage might vary based on Oracle version and settings
        overhead_bytes = 2  # Length bytes
        if self.char_semantics:
            return (self.char_length * 4) + overhead_bytes
        return self.byte_length + overhead_bytes
    
    def set_char_semantics(self, use_char_semantics: bool):
        """Set character semantics for VARCHAR column"""
        if self._is_varchar_type():
            self.char_semantics = use_char_semantics
            self._setup_varchar_metadata()
    
    def calculate_max_chars(self, sample_text: str = None) -> int:
        """Calculate maximum number of characters possible based on byte length"""
        if not self._is_varchar_type():
            return 0
            
        if self.char_semantics:
            return self.char_length
            
        if sample_text:
            # Calculate based on sample text encoding
            avg_bytes_per_char = len(sample_text.encode('utf8')) / len(sample_text)
            return int(self.byte_length / avg_bytes_per_char)
        
        # Conservative estimate (assuming UTF-8 encoding)
        return self.byte_length // 4

Usage example:
# Create a VARCHAR column with BYTE semantics
varchar_column = OracleColumn(
    field_name="employee_name",
    column_name="EMPLOYEE_NAME",
    schema_name="HR",
    table_name="EMPLOYEES",
    field_type="VARCHAR2",
    field_length="100",
    char_semantics=False,  # Use BYTE semantics
    comments="Employee full name"
)

# Create a VARCHAR column with CHAR semantics
varchar_column_char = OracleColumn(
    field_name="description",
    column_name="DESCRIPTION",
    schema_name="HR",
    table_name="EMPLOYEES",
    field_type="VARCHAR2",
    field_length="1000",
    char_semantics=True,  # Use CHAR semantics
    comments="Employee description"
)

# Get VARCHAR metadata
varchar_metadata = varchar_column.get_varchar_metadata()

# Generate DDL
ddl = varchar_column.get_ddl_definition()

# Estimate storage requirements
storage_bytes = varchar_column.estimate_storage_bytes()

# Calculate maximum possible characters
max_chars = varchar_column.calculate_max_chars()

# Set character semantics
varchar_column.set_char_semantics(True)

#############################################################
def parse_data_type(data_type_str):
    """
    Separates a data type string with length/precision into its components.
    Returns parameters as individual values, not in a list.
    
    Args:
        data_type_str (str): Input string like 'varchar(40)' or 'decimal(2,5)'
    
    Returns:
        tuple: Contains data_type and individual parameters
    """
    paren_start = data_type_str.find('(')
    
    if paren_start == -1:
        return (data_type_str.lower(),)
    
    base_type = data_type_str[:paren_start].lower()
    params_str = data_type_str[paren_start + 1:-1]
    parameters = [int(p.strip()) for p in params_str.split(',')]
    
    return (base_type, *parameters)

def process_column_definition(column_name, data_type_str):
    """
    Process a database column definition using the parse_data_type function.
    
    Args:
        column_name (str): Name of the column
        data_type_str (str): Data type string (e.g., 'varchar(40)')
    
    Returns:
        dict: Column information including name, type, and parameters
    """
    # Call parse_data_type function and handle its results
    type_info = parse_data_type(data_type_str)
    
    # Initialize column info dictionary
    column_info = {
        'column_name': column_name,
        'data_type': type_info[0]
    }
    
    # Add parameters based on data type
    if type_info[0] == 'varchar' or type_info[0] == 'char':
        if len(type_info) > 1:
            column_info['length'] = type_info[1]
    
    elif type_info[0] == 'decimal' or type_info[0] == 'numeric':
        if len(type_info) > 2:
            column_info['precision'] = type_info[1]
            column_info['scale'] = type_info[2]
    
    return column_info

# Example usage
def main():
    # Example 1: Processing a varchar column
    varchar_result = process_column_definition('first_name', 'varchar(40)')
    print("VARCHAR Column:")
    print(varchar_result)
    print()
    
    # Example 2: Processing a decimal column
    decimal_result = process_column_definition('price', 'decimal(2,5)')
    print("DECIMAL Column:")
    print(decimal_result)
    print()
    
    # Example 3: Processing a regular integer column
    int_result = process_column_definition('age', 'int')
    print("INTEGER Column:")
    print(int_result)

# Run the examples
main()
#############################################################
Here's how you can use this code:

1. Direct function call:
```python
# Simple usage
result = process_column_definition('username', 'varchar(50)')
print(result)
```

2. In a loop processing multiple columns:
```python
# Process multiple columns
columns = [
    ('name', 'varchar(40)'),
    ('price', 'decimal(2,5)'),
    ('age', 'int')
]

for column_name, data_type in columns:
    result = process_column_definition(column_name, data_type)
    print(result)
```

3. Within another function:
```python
def create_table_definition(table_name, columns):
    table_def = {'table_name': table_name, 'columns': []}
    
    for column_name, data_type in columns:
        column_info = process_column_definition(column_name, data_type)
        table_def['columns'].append(column_info)
    
    return table_def
```

The key points about calling the function within another function:

1. Make sure both functions are defined in the same scope or imported properly
2. Pass the parameters exactly as expected by the parse_data_type function
3. Handle the returned tuple appropriately in your wrapper function


