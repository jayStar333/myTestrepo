```python
def define_table_directly(args: argparse.Namespace):
    """
    Define a table directly with primary key information using Oracle metadata.
    
    :param args: Command line arguments containing schema and table information
    :return: None
    """
    source_identifier = ".".join([args.schema, args.table])
    query = f"SELECT * from {source_identifier}"
    print(query)
    
    authenticator = OracleAuthentication(**asdict(GL_SOURCES[args.source_of_record]))
    
    try:
        source_table = OracleTable(
            query=query,
            schema=args.schema,
            table_name=args.table,
            authenticator=authenticator
        )
        
        # Query to get primary key columns for this table
        pk_query = f"""
            SELECT column_name
            FROM all_cons_columns
            WHERE constraint_name = (
                SELECT constraint_name
                FROM all_constraints
                WHERE table_name = '{args.table}'
                AND owner = '{args.schema}'
                AND constraint_type = 'P'
            )
        """
        # Execute PK query using the same connection
        pk_columns = set(row[0] for row in source_table.connection.execute(pk_query))
        
    except AttributeError as error:
        print(f"{error=}")
        return

    print("before getting view")
    try:
        source_table.get_view_definition_table(schema=source_identifier)
    except Exception as error:
        print(f"{error=}")
        return

    # Include PK information in the output
    columns_info = []
    for column in source_table.all_columns:
        is_pk = "(PK)" if column.name in pk_columns else ""
        columns_info.append(f"{column.name}{is_pk}")

    print(f"table={source_identifier}|columns={len(source_table.all_columns)}|details={', '.join(columns_info)}")
    print("after getting view")
```
###################################
import os
import csv
import pandas as pd
from datetime import datetime

def process_csv_file(
    input_file_path, 
    output_directory, 
    selected_fields, 
    field_rename_mapping, 
    additional_field_generators
):
    """
    Process a CSV file by selecting specific fields, renaming them, 
    and adding derived fields.
    
    Parameters:
    - input_file_path (str): Full path to the input CSV file
    - output_directory (str): Directory where the output file will be saved
    - selected_fields (list): List of fields to extract from the original CSV
    - field_rename_mapping (dict): Mapping of old field names to new field names
    - additional_field_generators (list): List of functions to generate new fields
    
    Returns:
    - str: Path to the created metadata CSV file
    """
    # Ensure the output directory exists
    os.makedirs(output_directory, exist_ok=True)
    
    # Read the input CSV file
    try:
        df = pd.read_csv(input_file_path)
    except Exception as e:
        raise ValueError(f"Error reading input CSV file: {e}")
    
    # Validate and select specified fields
    selected_df = df[selected_fields].copy()
    
    # Rename fields
    selected_df.rename(columns=field_rename_mapping, inplace=True)
    
    # Generate additional fields
    for field_generator in additional_field_generators:
        field_name, generator_func = field_generator
        selected_df[field_name] = selected_df.apply(generator_func, axis=1)
    
    # Create metadata filename with timestamp
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    metadata_filename = f"metadata_{timestamp}.csv"
    metadata_filepath = os.path.join(output_directory, metadata_filename)
    
    # Write the processed data to CSV
    selected_df.to_csv(metadata_filepath, index=False)
    
    return metadata_filepath

# Example usage and demonstration
def example_usage():
    # Example input file path
    input_file = 'sample_data.csv'
    
    # Output directory
    output_dir = './processed_data'
    
    # Selected fields to extract
    selected_fields = ['name', 'age', 'salary', 'department']
    
    # Field renaming mapping
    field_rename_map = {
        'name': 'full_name', 
        'age': 'person_age', 
        'salary': 'annual_income',
        'department': 'work_unit'
    }
    
    # Additional field generators
    def calculate_tax_bracket(row):
        """Calculate tax bracket based on income"""
        salary = row['annual_income']
        if salary < 30000:
            return 'Low'
        elif 30000 <= salary < 80000:
            return 'Medium'
        else:
            return 'High'
    
    def generate_age_category(row):
        """Categorize age"""
        age = row['person_age']
        if age < 25:
            return 'Young'
        elif 25 <= age < 45:
            return 'Mid-Career'
        else:
            return 'Senior'
    
    def calculate_potential_bonus(row):
        """Calculate potential bonus based on age and income"""
        age_category = generate_age_category(row)
        tax_bracket = calculate_tax_bracket(row)
        
        if age_category == 'Senior' and tax_bracket == 'High':
            return row['annual_income'] * 0.15
        elif age_category == 'Mid-Career' and tax_bracket == 'Medium':
            return row['annual_income'] * 0.10
        else:
            return row['annual_income'] * 0.05
    
    def generate_employee_code(row):
        """Generate a unique employee code"""
        return f"{row['work_unit'][:3].upper()}-{row['person_age']}"
    
    # Additional field generator specification
    additional_fields = [
        ('tax_bracket', calculate_tax_bracket),
        ('age_category', generate_age_category),
        ('potential_bonus', calculate_potential_bonus),
        ('employee_code', generate_employee_code)
    ]
    
    # Process the CSV
    output_file = process_csv_file(
        input_file, 
        output_dir, 
        selected_fields, 
        field_rename_map, 
        additional_fields
    )
    
    print(f"Processed CSV saved to: {output_file}")

# Uncomment the line below to run the example
# example_usage()
###################################
import os
import csv
import pandas as pd
from datetime import datetime

def get_hive_datatype(oracle_type):
    """
    Convert Oracle datatype to Hive equivalent
    """
    oracle_type = str(oracle_type).upper().strip()
    
    # Mapping of Oracle to Hive datatypes
    datatype_mapping = {
        'VARCHAR2': 'STRING',
        'VARCHAR': 'STRING',
        'CHAR': 'STRING',
        'NCHAR': 'STRING',
        'NVARCHAR2': 'STRING',
        'CLOB': 'STRING',
        'NCLOB': 'STRING',
        'NUMBER': 'DECIMAL',
        'NUMERIC': 'DECIMAL',
        'DECIMAL': 'DECIMAL',
        'INTEGER': 'INT',
        'INT': 'INT',
        'SMALLINT': 'SMALLINT',
        'FLOAT': 'DOUBLE',
        'REAL': 'DOUBLE',
        'DOUBLE PRECISION': 'DOUBLE',
        'DATE': 'TIMESTAMP',
        'TIMESTAMP': 'TIMESTAMP',
        'BLOB': 'BINARY',
        'LONG': 'STRING',
        'LONG RAW': 'BINARY',
        'RAW': 'BINARY',
        'BOOLEAN': 'BOOLEAN'
    }
    
    # Handle NUMBER with precision and scale
    if 'NUMBER' in oracle_type:
        if '(' in oracle_type:
            precision_scale = oracle_type.split('(')[1].rstrip(')').split(',')
            if len(precision_scale) == 2:
                precision, scale = map(int, precision_scale)
                if scale == 0:
                    if precision < 5:
                        return 'SMALLINT'
                    elif precision < 10:
                        return 'INT'
                    elif precision < 19:
                        return 'BIGINT'
                    else:
                        return 'DECIMAL({},{})'.format(precision, scale)
                else:
                    return 'DECIMAL({},{})'.format(precision, scale)
    
    # Return mapped type or STRING as default
    return datatype_mapping.get(oracle_type.split('(')[0], 'STRING')

def process_csv_file(
    input_file_path, 
    output_directory, 
    selected_fields,
    field_rename_mapping,
    dev_prefix_field,      # Field to add dev_ prefix
    conf_prefix_field,     # Field to add conf_ prefix
    pass_through_field,    # Field to pass through as-is
    datatype_field        # Field containing Oracle datatype
):
    """
    Process a CSV file by selecting specific fields, renaming them, 
    and adding derived fields with specific prefixes and transformations.
    
    Parameters:
    - input_file_path (str): Full path to the input CSV file
    - output_directory (str): Directory where the output file will be saved
    - selected_fields (list): List of fields to extract from the original CSV
    - field_rename_mapping (dict): Mapping of old field names to new field names
    - dev_prefix_field (str): Field to be prefixed with 'dev_'
    - conf_prefix_field (str): Field to be prefixed with 'conf_'
    - pass_through_field (str): Field to be passed through unchanged
    - datatype_field (str): Field containing Oracle datatype to be converted to Hive
    
    Returns:
    - str: Path to the created metadata CSV file
    """
    # Ensure the output directory exists
    os.makedirs(output_directory, exist_ok=True)
    
    try:
        # Read the input CSV file
        df = pd.read_csv(input_file_path)
        
        # Validate and select specified fields
        selected_df = df[selected_fields].copy()
        
        # Rename fields
        selected_df.rename(columns=field_rename_mapping, inplace=True)
        
        # Add dev_ prefix column
        if dev_prefix_field in selected_df.columns:
            new_dev_field = f'dev_{dev_prefix_field}'
            selected_df[new_dev_field] = 'dev_' + selected_df[dev_prefix_field].astype(str)
        
        # Add conf_ prefix column
        if conf_prefix_field in selected_df.columns:
            new_conf_field = f'conf_{conf_prefix_field}'
            selected_df[new_conf_field] = 'conf_' + selected_df[conf_prefix_field].astype(str)
        
        # Add pass-through column
        if pass_through_field in selected_df.columns:
            new_pass_field = f'new_{pass_through_field}'
            selected_df[new_pass_field] = selected_df[pass_through_field]
        
        # Add Hive datatype column
        if datatype_field in selected_df.columns:
            new_type_field = f'hive_{datatype_field}'
            selected_df[new_type_field] = selected_df[datatype_field].apply(get_hive_datatype)
        
        # Create metadata filename with timestamp
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        metadata_filename = f"metadata_{timestamp}.csv"
        metadata_filepath = os.path.join(output_directory, metadata_filename)
        
        # Write the processed data to CSV
        selected_df.to_csv(metadata_filepath, index=False)
        
        return metadata_filepath
        
    except Exception as e:
        raise ValueError(f"Error processing CSV file: {e}")

# Example usage and demonstration
def example_usage():
    # Example input file path
    input_file = 'sample_data.csv'
    
    # Output directory
    output_dir = './processed_data'
    
    # Selected fields to extract
    selected_fields = ['column_name', 'data_type', 'table_name', 'schema_name', 'column_id']
    
    # Field renaming mapping
    field_rename_map = {
        'column_name': 'col_name',
        'data_type': 'oracle_type',
        'table_name': 'tbl_name',
        'schema_name': 'schema',
        'column_id': 'col_id'
    }
    
    # Process the CSV
    output_file = process_csv_file(
        input_file_path=input_file,
        output_directory=output_dir,
        selected_fields=selected_fields,
        field_rename_mapping=field_rename_map,
        dev_prefix_field='tbl_name',    # Will create 'dev_tbl_name'
        conf_prefix_field='schema',      # Will create 'conf_schema'
        pass_through_field='col_name',   # Will create 'new_col_name'
        datatype_field='oracle_type'     # Will create 'hive_oracle_type'
    )
    
    print(f"Processed CSV saved to: {output_file}")

# Example of input CSV structure:
"""
column_name,data_type,table_name,schema_name,column_id
id,NUMBER(10),customers,sales,1
name,VARCHAR2(100),customers,sales,2
email,VARCHAR2(255),customers,sales,3
created_date,TIMESTAMP,customers,sales,4
balance,NUMBER(10,2),customers,sales,5
"""

# Example of output CSV structure:
"""
col_name,oracle_type,tbl_name,schema,col_id,dev_tbl_name,conf_schema,new_col_name,hive_oracle_type
id,NUMBER(10),customers,sales,1,dev_customers,conf_sales,id,INT
name,VARCHAR2(100),customers,sales,2,dev_customers,conf_sales,name,STRING
email,VARCHAR2(255),customers,sales,3,dev_customers,conf_sales,email,STRING
created_date,TIMESTAMP,customers,sales,4,dev_customers,conf_sales,created_date,TIMESTAMP
balance,NUMBER(10,2),customers,sales,5,dev_customers,conf_sales,balance,DECIMAL(10,2)
"""

# Uncomment the line below to run the example
# example_usage()
